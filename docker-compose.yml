version: '3.8'

services:
  triton:
    image: nvcr.io/nvidia/tritonserver:25.06-py3
    container_name: triton_inference_server
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./model_repository:/models
    ports:
      - "8000:8000"   # HTTP
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics
    command: >
      tritonserver --model-repository=/models --model-control-mode explicit

  owlban:
    build:
      context: ./external/owlbangroup.io
    container_name: owlban_group_platform
    ports:
      - "3000:3000"
      - "8080:8080"
    environment:
      - NODE_ENV=production
      - PORT=3000
    depends_on:
      - triton
